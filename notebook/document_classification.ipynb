{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwF1vTI4z0Ac"
   },
   "source": [
    "Работа сделана на основе статьи Yoon Kim [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lf4kAMCsGK9",
    "outputId": "a0724eb8-933c-490e-b60b-bed314cac6b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "seed = 8765\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FNwNy0xthA9"
   },
   "source": [
    "### Загрузка и предобработка данных\n",
    "Загружаем данные в модель и проводим предобработку, для каждого экземпляра получаем:\n",
    "* Вопрос\n",
    "* Категория"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rBY4rgibt3tj",
    "outputId": "e854757b-4992-43b2-c43c-7f2da80b675e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                             Description     Category\n",
       "0     hi since recruiter lead permission approve req...  Application\n",
       "1     re expire days hi ask help update passwords co...  Application\n",
       "2     verification warning hi has got attached pleas...  Application\n",
       "3     please dear looks blacklisted receiving mails ...  Application\n",
       "4     dear modules report report cost thank much reg...  Application\n",
       "...                                                 ...          ...\n",
       "2995  sent tuesday feedback follow up conf call hell...     Database\n",
       "2996  sent monday issues hi keep getting errors whil...     Database\n",
       "2997  sent monday en working properly hi guys we hav...     Database\n",
       "2998  sent wednesday july hi please log incident for...     Database\n",
       "2999  sent tuesday july connection issues hello have...     Database\n",
       "\n",
       "[3000 rows x 2 columns]>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('latest_ticket_data.csv')\n",
    "\n",
    "display(data.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0V6YAt_vEYq"
   },
   "source": [
    "### Конвертирование данных обучение/тестовые в pd.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "aQ2YrLbKvVvR",
    "outputId": "0cb911b2-9690-43fb-819d-7f54caff8875"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>re maternity leaver form hi please find attach...</td>\n",
       "      <td>User Maintenance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>column added hi please rise assign thanks best...</td>\n",
       "      <td>Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>sent thursday october vs problems hi have inst...</td>\n",
       "      <td>Database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>sent saturday event notification data event th...</td>\n",
       "      <td>Security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>wednesday hey va milk fruits fresh la care si ...</td>\n",
       "      <td>User Maintenance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>sent friday november issue with hi guys assist...</td>\n",
       "      <td>Database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>sent tuesday october printer error th floor to...</td>\n",
       "      <td>Database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>ports allow connecting hello please allow traf...</td>\n",
       "      <td>Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182</th>\n",
       "      <td>sent friday lost badge hello lost badge notice...</td>\n",
       "      <td>Security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>re maternity form hi please attached filled fo...</td>\n",
       "      <td>User Maintenance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Description          Category\n",
       "1266  re maternity leaver form hi please find attach...  User Maintenance\n",
       "429   column added hi please rise assign thanks best...       Application\n",
       "2909  sent thursday october vs problems hi have inst...          Database\n",
       "2307  sent saturday event notification data event th...          Security\n",
       "1599  wednesday hey va milk fruits fresh la care si ...  User Maintenance\n",
       "2722  sent friday november issue with hi guys assist...          Database\n",
       "2851  sent tuesday october printer error th floor to...          Database\n",
       "1008  ports allow connecting hello please allow traf...           Network\n",
       "2182  sent friday lost badge hello lost badge notice...          Security\n",
       "1226  re maternity form hi please attached filled fo...  User Maintenance"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(data, test_size=0.2)\n",
    "\n",
    "train_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fh9ksWJuvwm3"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1.0, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oF7885U_v2KF"
   },
   "source": [
    "### Конвертация строковых меток в целочисленные ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "ilfXRXaDvx5M",
    "outputId": "8cfdbb9b-a304-4b98-db1f-e8d70463f877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label->ID mapping: {'Security': 0, 'User Maintenance': 1, 'Database': 2, 'Application': 3, 'Network': 4}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>access doors dear please investigate with high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>badge needed floor hi please floor thank</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2984</th>\n",
       "      <td>sent friday working issue hi having similar is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>sent october lost access card hi our colleague...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>database backup needed hello backup database k...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>sent monday trouble connecting monitor work st...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>purchase po dear purchased cable type apple li...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>sent friday availability importance high hi so...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>friday fwd personal weekend bun conform solici...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>sent friday recovery key dear trouble with his...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Description  Category\n",
       "1986  access doors dear please investigate with high...         0\n",
       "1235           badge needed floor hi please floor thank         1\n",
       "2984  sent friday working issue hi having similar is...         2\n",
       "2390  sent october lost access card hi our colleague...         0\n",
       "526   database backup needed hello backup database k...         3\n",
       "2975  sent monday trouble connecting monitor work st...         2\n",
       "721   purchase po dear purchased cable type apple li...         2\n",
       "2551  sent friday availability importance high hi so...         4\n",
       "1723  friday fwd personal weekend bun conform solici...         1\n",
       "2959  sent friday recovery key dear trouble with his...         2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_cats = train_df[\"Category\"].unique()\n",
    "labels_map = dict(zip(unique_cats, np.arange(unique_cats.shape[0])))\n",
    "print(f\"Label->ID mapping: {labels_map}\")\n",
    "\n",
    "n_classes = len(labels_map)\n",
    "\n",
    "train_df[\"Category\"] = train_df[\"Category\"].map(labels_map)\n",
    "test_df[\"Category\"] = test_df[\"Category\"].map(labels_map)\n",
    "\n",
    "train_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0vBwuUDwP6-"
   },
   "source": [
    "### Разделение данных на группы для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "a7GryzcFwigg",
    "outputId": "56f46544-de7c-4410-d045-3219c9bb472c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (1920, 2)\n",
      "Valid size: (480, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>sent friday lost badge hello one our colleague...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>sent thursday march re ny speed attached north...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>lost access card hi lost access card could you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2427</th>\n",
       "      <td>sent tuesday march issue hi could you please h...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>sent friday lost badge hello today around lost...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Description  Category\n",
       "2151  sent friday lost badge hello one our colleague...         0\n",
       "2434  sent thursday march re ny speed attached north...         4\n",
       "1978  lost access card hi lost access card could you...         0\n",
       "2427  sent tuesday march issue hi could you please h...         4\n",
       "2173  sent friday lost badge hello today around lost...         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, valid_df = train_test_split(train_df, test_size=0.2)\n",
    "print(f\"Train size: {train_df.shape}\")\n",
    "print(f\"Valid size: {valid_df.shape}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOms1ObZxHVV"
   },
   "source": [
    "### Токенизация\n",
    "Определение токенизатора с обучаемой выборкой и размера словаря, который соответствует размеру словаря `index_word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lW1ObpvixMHa",
    "outputId": "f61893b3-8fae-4da5-cb74-79ba015dd046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4365\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer and fit on train data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df[\"Description\"].tolist())\n",
    "\n",
    "# Derive the vocabulary size\n",
    "n_vocab = len(tokenizer.index_word) + 1\n",
    "print(f\"Vocabulary size: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6zCGng_x6DH"
   },
   "source": [
    "### Длина последовательности\n",
    "Здесь проводим анализ `1%` и `99%` персентиля последовательностей. Используем `99%` персентиля как максимальная длина последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJb-rJSqxthz",
    "outputId": "f4ae16c5-eaa5-4d1d-ec07-4f04ccd30480"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1920.000000\n",
       "mean       35.926042\n",
       "std        57.074974\n",
       "min         1.000000\n",
       "1%          3.000000\n",
       "50%        21.000000\n",
       "99%       260.150000\n",
       "max       927.000000\n",
       "Name: Description, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"Description\"].str.split(\" \").str.len().describe(percentiles=[0.01, 0.5, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4kp4l-wyero"
   },
   "source": [
    "### Выравнивание коротких предложений\n",
    "Выравниваем, чтобы все предложения были одинаковой длины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "M0tMve24yyzr"
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_df[\"Description\"].tolist())\n",
    "train_labels = train_df[\"Category\"].values\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_df[\"Description\"].tolist())\n",
    "valid_labels = valid_df[\"Category\"].values\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df[\"Description\"].tolist())\n",
    "test_labels = test_df[\"Category\"].values\n",
    "\n",
    "max_seq_length = 279\n",
    "\n",
    "preprocessed_train_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "preprocessed_valid_sequences = tf.keras.preprocessing.sequence.pad_sequences(valid_sequences, maxlen=max_seq_length, padding='post', truncating='post')\n",
    "preprocessed_test_sequences = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_seq_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5osBcJCzFeN"
   },
   "source": [
    "### Классификация предложений при помощи CNN\n",
    "Реализация простой CNN для классификации предложений (документов). Нейронная сеть имеет один слой, за ним следует слой `pooling-over-time` (термин из статьи и перевод неизвестен) и полносвязный слой с функцией активации `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59cS0CUfzEzX",
    "outputId": "1b46a283-e628-46a4-b652-6864e14d13be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 279)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 279, 80)      349200      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 279, 150)     60150       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 279, 150)     84150       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 279, 150)     132150      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 279, 450)     0           conv1d[0][0]                     \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 450)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 450)          0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 5)            2255        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 627,905\n",
      "Trainable params: 627,905\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 23:06:43.661260: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-30 23:06:43.662359: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-30 23:06:43.882223: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:43.882323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:07:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.785GHz coreCount: 40 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2023-08-30 23:06:43.882347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-30 23:06:43.883471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-30 23:06:43.883498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-30 23:06:43.884870: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-30 23:06:43.885043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-30 23:06:43.886278: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-30 23:06:43.886844: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-30 23:06:43.889265: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-30 23:06:43.889761: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:43.889987: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:43.890091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-08-30 23:06:43.891041: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-30 23:06:43.894207: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-30 23:06:43.894820: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:43.894848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:07:00.0 name: NVIDIA GeForce RTX 2070 SUPER computeCapability: 7.5\n",
      "coreClock: 1.785GHz coreCount: 40 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s\n",
      "2023-08-30 23:06:43.894886: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-30 23:06:43.894912: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-30 23:06:43.894926: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-30 23:06:43.894936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-30 23:06:43.894946: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-30 23:06:43.894955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-30 23:06:43.894964: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-30 23:06:43.894973: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-30 23:06:43.895182: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:43.895390: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:43.895402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-08-30 23:06:43.895436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-30 23:06:44.698886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-30 23:06:44.698923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-08-30 23:06:44.698927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-08-30 23:06:44.731158: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:44.731206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1489] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-30 23:06:44.731655: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:44.731951: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:927] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-30 23:06:44.731969: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-08-30 23:06:44.732022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6571 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:07:00.0, compute capability: 7.5)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Input layer takes word IDs as inputs\n",
    "word_id_inputs = layers.Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "# Embeddings of the inputs / out [batch_size, sent_length, output_dim]\n",
    "embedding_out = layers.Embedding(input_dim=n_vocab, output_dim=80)(word_id_inputs)\n",
    "\n",
    "\n",
    "# All layers: in [batch_size, sent_length, emb_size] / out [batch_size, sent_length, 150]\n",
    "conv1_1 = layers.Conv1D(\n",
    "    150, kernel_size=5, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "conv1_2 = layers.Conv1D(\n",
    "    150, kernel_size=7, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "conv1_3 = layers.Conv1D(\n",
    "    150, kernel_size=11, strides=1, padding='same', activation='relu'\n",
    ")(embedding_out)\n",
    "\n",
    "# in previous conve outputs / out [batch_size, sent_length, 450]\n",
    "conv_out = layers.Concatenate(axis=-1)([conv1_1, conv1_2, conv1_3])\n",
    "\n",
    "# Pooling over time operation. This is doing the max pooling over sequence lenth\n",
    "# in other words, each feature map results in a single output\n",
    "# in [batch_size, sent_length, 450] / out [batch_size, 1, 450]\n",
    "pool_over_time_out = layers.MaxPool1D(pool_size=max_seq_length, padding='valid')(conv_out)\n",
    "\n",
    "# Flatten the unit length dimension\n",
    "flatten_out = layers.Flatten()(pool_over_time_out)\n",
    "\n",
    "# Compute the final output\n",
    "out = layers.Dense(\n",
    "    n_classes, activation='softmax',\n",
    "    kernel_regularizer=regularizers.l2(0.001)\n",
    ")(flatten_out)\n",
    "\n",
    "# Define the model\n",
    "cnn_model = Model(inputs=word_id_inputs, outputs=out)\n",
    "\n",
    "# Compile the model with loss/optimzier/metrics\n",
    "cnn_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDB_MHEW14mj"
   },
   "source": [
    "### Обучение модели\n",
    "Обучение проводится с определенным значением батча по каждой эпохе. Используется TF колбэк `ReduceLROnPlateau` - уменьшает коэфициент обучения (learning rate) если нет улучшений в оценке качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0iFgPr02ch6",
    "outputId": "f55b0a37-e2e0-4c3f-eaf8-e77d75b34079"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 23:06:44.810291: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-08-30 23:06:44.811171: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3792910000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 23:06:45.271109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-30 23:06:45.487498: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 3s 57ms/step - loss: 1.6052 - accuracy: 0.2814 - val_loss: 1.5198 - val_accuracy: 0.5542\n",
      "Epoch 2/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 1.4400 - accuracy: 0.6806 - val_loss: 1.2772 - val_accuracy: 0.6333\n",
      "Epoch 3/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 1.1300 - accuracy: 0.7983 - val_loss: 0.9641 - val_accuracy: 0.6771\n",
      "Epoch 4/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.7897 - accuracy: 0.8091 - val_loss: 0.7881 - val_accuracy: 0.7312\n",
      "Epoch 5/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.5429 - accuracy: 0.8734 - val_loss: 0.7069 - val_accuracy: 0.7479\n",
      "Epoch 6/25\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.3728 - accuracy: 0.9186 - val_loss: 0.6781 - val_accuracy: 0.7667\n",
      "Epoch 7/25\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.2634 - accuracy: 0.9446 - val_loss: 0.6758 - val_accuracy: 0.7667\n",
      "Epoch 8/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.1912 - accuracy: 0.9626 - val_loss: 0.7134 - val_accuracy: 0.7625\n",
      "Epoch 9/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.1488 - accuracy: 0.9727 - val_loss: 0.7176 - val_accuracy: 0.7625\n",
      "Epoch 10/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.1145 - accuracy: 0.9856 - val_loss: 0.7509 - val_accuracy: 0.7542\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/25\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0956 - accuracy: 0.9888 - val_loss: 0.7505 - val_accuracy: 0.7563\n",
      "Epoch 12/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.1022 - accuracy: 0.9869 - val_loss: 0.7495 - val_accuracy: 0.7542\n",
      "Epoch 13/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0894 - accuracy: 0.9929 - val_loss: 0.7532 - val_accuracy: 0.7542\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 14/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.1020 - accuracy: 0.9875 - val_loss: 0.7534 - val_accuracy: 0.7542\n",
      "Epoch 15/25\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0894 - accuracy: 0.9929 - val_loss: 0.7536 - val_accuracy: 0.7542\n",
      "Epoch 16/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0964 - accuracy: 0.9902 - val_loss: 0.7538 - val_accuracy: 0.7542\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 17/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0921 - accuracy: 0.9910 - val_loss: 0.7539 - val_accuracy: 0.7542\n",
      "Epoch 18/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0904 - accuracy: 0.9921 - val_loss: 0.7539 - val_accuracy: 0.7542\n",
      "Epoch 19/25\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.0968 - accuracy: 0.9889 - val_loss: 0.7539 - val_accuracy: 0.7542\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 20/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0931 - accuracy: 0.9908 - val_loss: 0.7539 - val_accuracy: 0.7542\n",
      "Epoch 21/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0910 - accuracy: 0.9907 - val_loss: 0.7539 - val_accuracy: 0.7542\n",
      "Epoch 22/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0871 - accuracy: 0.9937 - val_loss: 0.7540 - val_accuracy: 0.7542\n",
      "Epoch 23/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0986 - accuracy: 0.9877 - val_loss: 0.7540 - val_accuracy: 0.7542\n",
      "Epoch 24/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0899 - accuracy: 0.9898 - val_loss: 0.7540 - val_accuracy: 0.7542\n",
      "Epoch 25/25\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.0884 - accuracy: 0.9911 - val_loss: 0.7540 - val_accuracy: 0.7542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f61c44a9d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Callback\n",
    "lr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=3, verbose=1,\n",
    "    mode='auto', min_delta=0.0001, min_lr=0.000001\n",
    ")\n",
    "\n",
    "cnn_model.fit(\n",
    "    preprocessed_train_sequences, train_labels,\n",
    "    validation_data=(preprocessed_valid_sequences, valid_labels),\n",
    "    batch_size=128,\n",
    "    epochs=25,\n",
    "    callbacks=[lr_reduce_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qptnhRP424nP"
   },
   "source": [
    "### Проверка модели на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7C3E_rY92-FM",
    "outputId": "6db59efc-6792-4784-904f-ee4220d6d4e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 6ms/step - loss: 0.7617 - accuracy: 0.7617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.7616634964942932, 'accuracy': 0.7616666555404663}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(preprocessed_test_sequences, test_labels, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-30 23:07:01.337214: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cnn_model/assets\n"
     ]
    }
   ],
   "source": [
    "cnn_model.save('cnn_model', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 279)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1251,    5,    1,   78, 1251,  110,    4,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_test_sequences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'charger hello please provide charger today thank'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"Description\"].tolist()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_dict.json' , 'w') as file:\n",
    "    json.dump(tokenizer.word_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
